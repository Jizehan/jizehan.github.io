## ES部署集群记录

1. 首先下载工具包进行解压，数据服务需要启动logstash，其他服务器只需要es和kibana即可

    ```bash
    wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-linux-x86_64.tar.gz

    wget https://artifacts.elastic.co/downloads/kibana/kibana-7.13.4-linux-x86_64.tar.gz

    wget https://artifacts.elastic.co/downloads/logstash/logstash-7.13.4-linux-x86_64.tar.gz
    ```

2. 解压到服务器/data/目录下之后，首先进入elaticsearch中的bin文件夹

    > 由于elasticsearch的安全机制，不可以使用root权限启动，所以如果是root身份的话，需要创建新的用户来启动
    >
    > ```bash
    > #添加elk用户组
    > groupadd elk
    > #添加elk用户，并关联到elk用户组中
    > useradd elk -g elk -p 123456
    > #将es文件夹和kibana文件夹的权限赋予给elk用户
    > chown -R elk:elk elasticsearch-7.13.4/
    > chown -R elk:elk kibana-7.13.4/
    > #切换到elk用户
    > su elk
    > ```
    >
    > 此时即可使用elk用户身份启动elasticsearch和kibana了
    > 进行其他操作时退出elk用户即可

    es对系统的两个参数有要求，最好进行修改

    > 两个报错信息如下：
    >
    > max file descriptors [4096] for elasticsearch process is  too low, increase to at least [65535]
    >
    > max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]

    修改方式：

    1. `max file descriptors [4096] for elasticsearch process is  too low, increase to at least [65535]`

        编辑/etc/security/limits.conf，追加以下内容；

        ```bash
        * soft nofile 65536
        * hard nofile 65536
        ```
		
        此文件修改后需要重新登录用户，才会生效

        重新登录后使用ulimit -S -n或者ulimit -H -n查看

    2. `max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]`

        使用`sysctl -a | grep vm.max_map_count`查看当前大小

        使用`sysctl -w vm.max_map_count=262144`修改为262144

        ```bash
        [root@BJIDC-CMP-ES01 ~] sysctl -w vm.max_map_count=262144
        vm.max_map_count = 262144
        ```

        最后可以使用`sysctl -a | grep vm.max_map_count`进行检查是否修改

3. 接下来进入config文件夹修改elasticsearch.yml文件

    1. 修改集群名称

        `cluster.name: hwzxes`

        > 注意：这里的集群名称要求整个集群所有机器上的配置文件中的cluster.name需要相同的名称

    2. 修改节点名称

        `node.name: node-bj`

        > 修改当前节点的名称

    3. 修改监听IP地址

        `network.host: 0.0.0.0`

        > 放开所有IP访问

    4. 修改端口

        `http.port: 9200`

        > 可以根据实际情况修改，如果不放开注释，默认端口为9200

    5. 填写其他节点的服务器host地址

        `discovery.seed_hosts: ["ip1", "ip2","ip3"]`

        > 根据实际节点进行修改，数组格式(注意：数组中要包含本机IP)

    6. 填写其他节点的服务地址

        `cluster.initial_master_nodes: ["ip1:9200", "ip2:9200","ip3:9200"]`

        > 根据实际节点进行修改，数组格式(注意：数组中要包含本机服务地址)

    7. 最后在配置文件末尾添加允许跨域的配置

        ```bash
        http.cors.enabled: true
        http.cors.allow-origin: "*"
        ```

    所有的node节点都进行此操作后进行下一步

4. 切换用户，进行启动

    ```
    su elk
    ./elasticsearch
    ```

    ```
    创建索引

    PUT /yzsb_index
    {
      "settings": {
        "number_of_shards": 4,(有几台机器)
        "number_of_replicas": 3（机器数减1）
      }
    }
    ```

## kibana部署

1. 进入config文件夹，修改kibana.yml文件

    1. 修改监听IP地址

        `server.host: "0.0.0.0"`

    2. 修改集群中的es节点地址
        `elasticsearch.hosts: ["ip1:9200", "1p2:9200", "1p3:9200"]`




tail -f logs/集群名.log

修改config/jvm.options
放开内存设置
```
-Xms4g
-Xmx4g
```
